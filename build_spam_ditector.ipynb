{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L06.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajkc-dev/great_courses_ml/blob/master/build_spam_ditector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-H4o3Mtf8I1"
      },
      "source": [
        "# **In this noteobook**, we will build a spam detector using two different models, a decision tree and a naive Bayes model. A naive Bayes classifier calculates the probability of a sequence of words belonging to a class as proprotional to the product of the probability of each item in a sequence given the class.\n",
        "\n",
        "Below we import the libraries we'll be using.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysfO6y4TygzU"
      },
      "source": [
        "from sklearn import tree\n",
        "import graphviz\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-flSIqbs4us"
      },
      "source": [
        "Next we'll write a function to process the data into a dictionary of words and their number of occurances, `word_dict`, and a count of the number of words total, `lexiconsize`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwlVeKj9GLFx"
      },
      "source": [
        "# read in the vocabulary file\n",
        "def readvocab(vocab_path=\"vocab.txt\"):\n",
        "   # keep track of the number of words\n",
        "    lexiconsize = 0\n",
        "   # initialize an empty dictionary\n",
        "    word_dict = {}\n",
        "   # create a feature for unknown words\n",
        "    word_dict[\"@unk\"] = lexiconsize\n",
        "    lexiconsize += 1\n",
        "   # read in the vocabular file\n",
        "    with open(vocab_path, \"r\") as f:\n",
        "        data = f.readlines()\n",
        "   # Process the file a line at a time.\n",
        "    for line in data:\n",
        "        # The count is the first 3 characters\n",
        "        count = int(line[0:4])\n",
        "        # The word is the rest of the string\n",
        "        token = line[5:-1]\n",
        "       # Create a feature if itâ€™s appeared at least twice\n",
        "        if count > 1:\n",
        "            word_dict[token] = lexiconsize\n",
        "            lexiconsize += 1\n",
        "    # squirrel away the total size for later reference\n",
        "    word_dict[\"@size\"] = lexiconsize\n",
        "    return(word_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M49cH5GGxTuG"
      },
      "source": [
        "We will download the vocabulary data from GitHub, `vocab.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TZ7Eft7CoRS"
      },
      "source": [
        "!wget https://github.com/mlittmancs/great_courses_ml/raw/master/data/vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_XRUDS34IL"
      },
      "source": [
        "Next, we write a `tokenize` function to turn each word into a list of the length of the number words.  Every item in the list is a count of the number of times a given word occurs in the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg0eeOYEbKBP"
      },
      "source": [
        "# Turn string str into a vector.\n",
        "def tokenize(email_string, word_dict):\n",
        "  # initially the vector is all zeros\n",
        "  vec = [0 for i in range(word_dict[\"@size\"])]\n",
        "  # for each word\n",
        "  for t in email_string.split(\" \"):\n",
        "   # if the word has a feature, add one to the corresponding feature\n",
        "    if t in word_dict: vec[word_dict[t]] += 1\n",
        "   # otherwise, count it as an unk\n",
        "    else: vec[word_dict[\"@unk\"]] += 1\n",
        "  return(vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF_Qo8nEJijp"
      },
      "source": [
        "From here, we write a `getdat` function to convert the file we downloaded into two lists:\n",
        "\n",
        "- `dat`: a list of lists of tokenized words\n",
        "- `labs`: a list of labels associated with the email being spam or not spam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "821K7VKScgx5"
      },
      "source": [
        "# read in labeled examples and turn the strings into vectors\n",
        "def getdat(filename, word_dict):\n",
        "    with open(filename, \"r\") as f:\n",
        "        data = f.readlines()\n",
        "    dat = []\n",
        "    labs = []\n",
        "    for line in data:\n",
        "        labs = labs + [int(line[0])]\n",
        "        dat = dat + [tokenize(line[2:], word_dict)]\n",
        "    return(dat, labs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcrtGdGMcqEg"
      },
      "source": [
        "Now we'll download the train and test data from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpS1CVviDFIq"
      },
      "source": [
        "!wget https://github.com/mlittmancs/great_courses_ml/raw/master/data/spam-test.csv\n",
        "!wget https://github.com/mlittmancs/great_courses_ml/raw/master/data/spam-train.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPPYYmSxdTk_"
      },
      "source": [
        "With these train and test datasets, we'll build create the data and labels we will use to train and use to test our naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIXOxbdFDpK4"
      },
      "source": [
        "word_dict = readvocab()\n",
        "traindat, trainlabs = getdat(\"spam-train.csv\", word_dict)\n",
        "testdat, testlabs = getdat(\"spam-test.csv\", word_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kNVQA_LhdE6"
      },
      "source": [
        "With the training and testing data, we can fit a decision tree with 6 decision rules and print out the accuracy on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qX25q8kgdqS3"
      },
      "source": [
        "clf = tree.DecisionTreeClassifier(max_leaf_nodes = 6)\n",
        "clf = clf.fit(traindat, trainlabs)\n",
        "\n",
        "yhat = clf.predict(testdat)\n",
        "\n",
        "sum([yhat[i] == testlabs[i] for i in range(len(testdat))])/len(testdat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLpCR7GXipHG"
      },
      "source": [
        "We now will create a list of the words in our wordlist and use it to print the decision tree we have learned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIA3MiRfeCGx"
      },
      "source": [
        "wordlist = list(word_dict.keys())[:-1]\n",
        "dot_data = tree.export_graphviz(clf, feature_names=wordlist,\n",
        "                      filled=True, rounded=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uB28SYZi0jF"
      },
      "source": [
        "How does the number of decision rules affect the accuracy of the model? We'll retrain the model 29 times to see how the accuracy changes as we increase the number of decision rules from 2 to 30."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbZM0Vj4eCkV"
      },
      "source": [
        "for leaves in range(2, 31):\n",
        "  clf = tree.DecisionTreeClassifier(max_leaf_nodes = leaves)\n",
        "  clf = clf.fit(traindat, trainlabs)\n",
        "  yhat = clf.predict(testdat)\n",
        "  acc = sum([yhat[i] == testlabs[i] for i in range(len(testdat))])/len(testdat)\n",
        "  print(leaves,acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzPXwVdEjM2b"
      },
      "source": [
        "Let's now fit a naive Bayes model and print the accuracy of the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKyZtRHgkWJT"
      },
      "source": [
        "clf = MultinomialNB().fit(traindat, trainlabs)\n",
        "clf = clf.fit(traindat, trainlabs)\n",
        "yhat = clf.predict(testdat)\n",
        "acc = sum([yhat[i] == testlabs[i] for i in range(len(testdat))])/len(testdat)\n",
        "acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tuVd6zVpdvJ"
      },
      "source": [
        "We can also calculate the confusion matrix of the model, a table of the following counts:\n",
        "\n",
        "True Negatives False Positives\n",
        "\n",
        "False Negatives True Positives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usa3eGx6eK4w"
      },
      "source": [
        "print(confusion_matrix(testlabs, yhat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v91_evRmkMvx"
      },
      "source": [
        "Let's visualize how Naive Bayes combines information from words in a sentence to make a judgement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpNi9hRalhK6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def plotsentence(sentence, clf):\n",
        "  acc = 1.0\n",
        "  labs = []\n",
        "  facs = []\n",
        "  factor = np.exp(clf.class_log_prior_[0]- clf.class_log_prior_[1])\n",
        "  labs += [\"PRIOR\"]\n",
        "  facs += [factor]\n",
        "  acc *= factor\n",
        "  for w in sentence:\n",
        "    i = word_dict[w]\n",
        "    factor = np.exp(clf.feature_log_prob_[0][i]- clf.feature_log_prob_[1][i])\n",
        "    labs += [w]\n",
        "    facs += [factor]\n",
        "    acc *= factor\n",
        "  labs += [\"POST\"]\n",
        "  facs += [acc]\n",
        "  return((labs,facs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA6qDQQ8qque"
      },
      "source": [
        "(labs,facs) = plotsentence(['yo', 'come', 'over', 'carlos', 'will', 'be', 'here', 'soon'], clf)\n",
        "facs = [ fac if fac >= 1.0 else -1/fac for fac in facs ]\n",
        "[(l,round(f,1)) for (l,f) in zip(labs,facs)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmveTDZ4y_jo"
      },
      "source": [
        "(labs,facs) = plotsentence(['congratulations', 'thanks', 'to', 'a', 'good', 'friend', 'u', 'have', 'won'], clf)\n",
        "facs = [ fac if fac >= 1.0 else -1/fac for fac in facs ]\n",
        "[(l,round(f,1)) for (l,f) in zip(labs,facs)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}